---
title: "Introduction"
author: |
  | Author: Holiday Tang 
  | [LinkeDin](https://www.linkedin.com/in/holiday-t/) |  [GitHub](https://github.com/holiday007) | [Kaggle Novice](https://www.kaggle.com/holibae007)
date: "| Date: `r Sys.Date()`"
output:
  github_document:
    toc: true
    fig_width: 7
    fig_height: 5
    toc_depth: 3
    dev: png
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F, comment=NA, cache=T)
```

#### Pending

* Interval Variables and Ratio Variables


**Types of Categorical Variables**

Necessarily discrete

* binary
* multinomial
* nominal - no natural ordering
* ordinal

Ordinal variables may lack numerical distances between the levels (unlike interval variables) or meaningful ratios (unlike ratio variables).

## Distributions

### Bernouli

* 1 trial

Some mathematical details 

### Binomial

Y: number of successes in n **independent** trials with success probability $\pi$

Some mathmetical details

In probability theory, the **central limit theorem** establishes that, in some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed.


### Multinomial

* n **idependent** trials, 

* each resulting in one of $c$ categories,

* with probabilities $\pi_1$, ..., $\pi_c$ (which sum to 1)

Notice multiple variables to one distribution

* So multiple variance & existence of covariance

### Poisson

* for counts with no obvious maximum

* Mean = Variance = count parameter

* there is a potential connection between multinomial and poisson (P8)



*Review also normal, chi-square, hyper-geometric distributions*
  
  * [Chi-Square](https://en.wikipedia.org/wiki/Chi-squared_distribution)
  
    - In probability theory and statistics, the chi-square distribution (also chi-squared or Ï‡2-distribution) with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables. The chi-square distribution is a special case of the gamma distribution and is one of the most widely used probability distributions in inferential statistics, notably in hypothesis testing and in construction of confidence intervals. When it is being distinguished from the more general noncentral chi-square distribution, this distribution is sometimes called the central chi-square distribution.

  - The chi-square distribution is used in the common chi-square tests for goodness of fit of an observed distribution to a theoretical one, the independence of two criteria of classification of qualitative data, and in confidence interval estimation for a population standard deviation of a normal distribution from a sample standard deviation. Many other statistical tests also use this distribution, such as Friedman's analysis of variance by ranks.
  
  * [Normal](https://en.wikipedia.org/wiki/Normal_distribution)
  * [Hyper-geometric](https://en.wikipedia.org/wiki/Hypergeometric_distribution)
  
## Aspects of likelihood

In statistics, the likelihood function (often simply called the likelihood) is formed from the joint probability of a sample of data given a set of model parameter values; it is viewed and used as a function of the parameters given the data sample.

The likelihood function describes a hypersurface whose peak, if it exists, represents the combination of model parameter values that maximize the probability of drawing the sample obtained.


* The kernel of $l(\beta)$ includes only factors that depend on $\beta$
  - $L(\beta)$ is \[log(l(\beta))\] 
  
* For our purposes, $L(\beta)$ will be well-defined and at least twice
continuously diferentiable.

* **Score Function**

  - expectation = 0

  - gradient of the log-likelihood function
  
  - often asymtotically normal

* **Fisher Information**
  
  - the variance of score function
  
  - a way of measuring the amount of information that an observable random variable X carries about an unknown parameter
  
  -typically, the reverse of the fisher's information is the asymptotic variance / covariance of MLE $\beta$
  
  
  
* (These can be found even when $L(\beta)$ is known only up
to an additive constant.)

## Inference

### Confidence Intervals

#### Score Intervals

- Explicit form at Agresti (1.14)

#### Wald Intervals

- Not reliable when n is small, or when the expected value is small?

#### Likelihood Intervals

- No explicit form

#### Clopper-Pearson "exact" CI

- Agrestic (16.6.1)

#### P-Value

- mid P-value (P44)

## Multinomial Inference

### Likelihood ratio test

- "$G^2$" is a test statistics for LR test

- converges slower

- don't use when n/c < 5, c is the number of categories

### Score test

- 