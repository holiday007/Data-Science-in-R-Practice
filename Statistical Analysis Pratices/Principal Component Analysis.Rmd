---
title: "Insert Title"
author: 'Holiday Tang'
date: "| Date: `r Sys.Date()`"
output:
  github_document:
    toc: true
    fig_width: 7
    fig_height: 5
    toc_depth: 2
    dev: png
---

Source: [Data Camp Article](https://www.datacamp.com/community/tutorials/pca-analysis-r)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F, comment=NA, fig.height = 5, fig.width = 7, cache = TRUE)
```

Data used = `mtcars` built in to R 

# Performing PCA

```{r}
mtcars.pca = prcomp(mtcars[, c(1:7, 10, 11)], center = T, scale. = T)
summary(mtcars.pca)
```

Proportion of variance refer to the extent, or percentage of variance explained by the PC.

```{r}
str(mtcars.pca)
```

- rotation: The relationship (correlation or anticorrelation, etc) between the initial variables and the principal components

- x: The values of each sample in terms of the principal components ($x)

# Visualizing PCA using biplot

```{r}
library(ggbiplot)

ggbiplot(mtcars.pca) + labs(x="PC1 (62.8%)", y="PC2 (23.1%)")
```

## Label

```{r}
ggbiplot(mtcars.pca, labels = rownames(mtcars))
```

By the clustering pattern, you can see what moedels of cars are similar to one another.

## Grouping the observations by countries

```{r}
mtcars.country = c(rep("Japan", 3), rep("US",4), rep("Europe", 7),rep("US",3), "Europe", rep("Japan", 3), rep("US",4), rep("Europe", 3), "US", rep("Europe", 3))

ggbiplot(mtcars.pca, labels = rownames(mtcars), groups = mtcars.country, ellipse = T)
```

- American cars are characterized by high `cyl`, `disp`, and `wt`, while Japanese care are characterized by high `mpg`

## Other PCs

```{r}
ggbiplot(mtcars.pca, labels=rownames(mtcars), groups=mtcars.country, choices = 3:4, ellipse = T)
```

## Customizing the plot

```{r}
ggbiplot(mtcars.pca,ellipse=TRUE,obs.scale = 1, var.scale = 1,
         labels=rownames(mtcars), groups=mtcars.country) + 
  scale_color_manual(name="Origin", values = c("forest green",
                                               "red3", 
                                               "dark blue")) + 
  theme_bw() + 
  theme(legend.position = "bottom")
```

Note:

* Extreme outlier have experme effects on PCA 

Example - let's add a extreme sample

```{r}
spacecar <- c(1000,60,50,500,0,0.5,2.5,0,1,0,0)

mtcarsplus <- rbind(mtcars, spacecar)
mtcars.countryplus <- c(mtcars.country, "Jupiter")


mtcarsplus.pca <- prcomp(mtcarsplus[,c(1:7,10,11)], center = TRUE,scale. = TRUE)

ggbiplot(mtcarsplus.pca, obs.scale = 1, var.scale = 1, ellipse = TRUE, circle = FALSE, var.axes=TRUE, labels=c(rownames(mtcars), "spacecar"), groups=mtcars.countryplus)+
  scale_colour_manual(name="Origin", values= c("forest green", "red3", "violet", "dark blue"))+
  ggtitle("PCA of mtcars dataset, with extra sample added")+
  theme_minimal()+
  theme(legend.position = "bottom")
```

If you want to see how the new sample compares to the groups produced by the initial PCA, you need to project it onto that PCA.

What this means is that the principal components are defined without relation to your spacecar sample, then you compute where spacecar is placed in relation to the other samples by applying the transformations that your PCA has produced.

1. scale the values for spacecar in relation to the PCA's center (`mtcars.pca$center`)

2. apply the rotation of the PCA matrix to the spacecar sampl3

3. `rbind()` the projected values for `spacecar` to the rest of the `pca$x`

```{r}
s.sc = scale(t(spacecar[c(1:7, 10:11)]), center=mtcars.pca$center)
s.pred = s.sc %*% mtcars.pca$rotation

mtcars.plusproj.pca <- mtcars.pca
mtcars.plusproj.pca$x <- rbind(mtcars.plusproj.pca$x, s.pred)

ggbiplot(mtcars.plusproj.pca, obs.scale = 1, var.scale = 1, ellipse = TRUE, circle = FALSE, var.axes=TRUE, labels=c(rownames(mtcars), "spacecar"), groups=mtcars.countryplus)+
  scale_colour_manual(name="Origin", values= c("forest green", "red3", "violet", "dark blue"))+
  ggtitle("PCA of mtcars dataset, with extra sample projected")+
  theme_minimal()+
  theme(legend.position = "bottom")
```

`Spacecar` is no longer skewing the PCA, but it is drastically different from every other model or group

* performing both PCA and PCA projection can be helpful EDA efforts
