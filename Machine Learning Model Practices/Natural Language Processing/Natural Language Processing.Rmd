---
title: "Natural Language Processing Practice"
author: |
  | Author: Holiday Tang
  | [LinkeDin](https://www.linkedin.com/in/holiday-t/) |  [GitHub](https://github.com/holiday007) | [Kaggle Novice](https://www.kaggle.com/holibae007)
date: "Date: `r Sys.Date()`"
output:
  github_document:
    toc: true
    fig_width: 7
    fig_height: 5
    toc_depth: 2
    dev: png
    pandoc_args: --webtex
---

(Note: adopted from *Data Scicence and Machine Learning Bootcamp with R* by *Jose Portilla* from Udemy) 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F, comment=NA, cache=T)

# install packages
packs = c("tm", "twitteR", "wordcloud", "RColorBrewer", "e1017", "class")

new.pkg <- packs[!(packs %in% installed.packages())]

if (length(new.pkg)) {
  install.packages(new.pkg, repos = "http://cran.rstudio.com")
}
```

# Introduction to natural language processing

Potential purpose:
  
  - Find relevant documents
  - Group articles by topic
  
We will want to:

* compile documents
* Featurize them
* Compare their features

Featurization:
  
  - one option is based on **word counts**
  - use consine similarity on the vectors made to determine similarity:

$$
sim(A,B) = cos(\theta) = \frac{A\cdot B}{||A||*||B||}
$$

We can improve on Bag of words by adjusting word counts based on their frequency in corpus, the group of all documents.

We can use TF-IDF (Term Frequency-Inverse Document Frequency) 

* Term frequency - **Importance** of the term within that document
  - TF(d,t) = Number of occurences of term t in document d

* Inverse document frequency - how important a word is to a document in a collection or corpus. 
  - IDF(t) = log(D/t) 
    - D = total number of documents 
    - t = number of documents with the term
    - so for term x within document y

$$
W_{x,y} = tf_{x,y} \cdot log(N/df_x)
$$

- $tf_{x,y}$ - frequency of x in y
- $df_x$ - number of documents containing x
- N - total number of documents

The tfâ€“idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.

```{r}
library(twitteR)
library(tm)
library(wordcloud)
library(RColorBrewer)
```

Connect to twitter

```{r}
ckey = "A7pB8aV2Gf7cTDjJDjBsmXCBY"
cskey = "lEwtyOQUq7rePc8XDIxnjC1oyxbuL3vcVkygsb499BMAk0jBR7"
token = "1183026577715744769-4cyyVR2UaJ4SBoSFcCcnqR0rzkJvWR"
sec.token = "K8rB8j9QxUVfChcqNJES4MydbV85rihzHbG0YkJZno0Rw"
setup_twitter_oauth(ckey, cskey, token, sec.token) # set up authorization
```

Search twitters

```{r}
# returning tweets
soccer.tweets = searchTwitter("soccer", 
                              n = 1000,
                              lang = "en")
```

```{r}
soccer.tweets[1:5]
```

```{r}
# grabbing text data from tweets
soccer.text = sapply(soccer.tweets, function(x) x$getText())
```

```{r}
soccer.text[1:5]
```

Clean Text Data (remove emoticons)

```{r}
# iconv() - an encoding function
soccer.text = iconv(soccer.text, 'UTF-8', "ASCII") # remove emoticons and characters that are not in utf8
soccer.text[1:5]
```

Create a corpus

```{r}
# VectorSource - creates a vector source from the text data
soccer.corpus = Corpus(VectorSource(soccer.text))
```

```{r}
soccer.corpus
```


Document term matrix

```{r}
term.doc.matrix = TermDocumentMatrix(soccer.corpus,
            # control - a list of actions to do on every document in the corpus
                                     control = list(removePunctuation = T,
            # stop words - really common words that do not add a lot of info, basically removing them here
                                                    stopwords = c("soccer",
                                                                  stopwords("en")),
                                                    removeNumbers = T, 
                                                    tolower=T))
```

Convert Term Document Matrix into an actual matrix

```{r}
term.doc.matrix = as.matrix(term.doc.matrix)
term.doc.matrix[1:3,1:3]
```

Get the word counts

```{r}
word.freq = sort(rowSums(term.doc.matrix), decreasing = T)
word.freq[1:5]
```

```{r}
dm = data.frame(word = names(word.freq), freq = word.freq)
```

Create the word cloud

```{r}
wordcloud(dm$word,dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
```